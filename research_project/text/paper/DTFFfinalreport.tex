\documentclass[12pt]{article}
\usepackage{amsfonts,amsmath,amssymb, listings}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xcolor}

\usepackage{threeparttable}
\usepackage{blindtext}


\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage[numbered,framed]{matlab-prettifier}
%\usepackage{color}

%\setcounter{MaxMatrixCols}{10}

\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}%

\newcommand{\fiid}{\func{i.i.d.}} % use this as in: X_i \stackrel{\fiid}{\sim} \func{Exp} \left( \lambda \right)
\DeclareMathOperator{\Norm}{N}

\newcommand{\platzo}{\vspace*{2cm}}
\newcommand{\platzt}{\vspace*{4cm}}

\newcommand{\findep}{\func{ind}}  % could use indep instead if ind, but 1st book uses ind.

\newcommand{\R}{\ensuremath{{\mathbb R}}}
\newcommand{\N}{\ensuremath{{\mathbb N}}}
\def\QATOP#1#2{{#1 \atop #2}}
\def\QTATOP#1#2{{\textstyle {#1 \atop #2}}}
\def\QDATOP#1#2{{\displaystyle {#1 \atop #2}}}
\voffset=-2.54cm\hoffset=-2.54cm \textheight27cm \textwidth17.0cm \topmargin0.5cm
\oddsidemargin2.00cm \evensidemargin2.00cm \unitlength1cm

\def\func#1{\mathop{\rm #1}}%
\def\dint{\mathop{\displaystyle \int}}%
\newcommand{\Ind}{\ensuremath{{\mathbb I}}} % indicator function
\newcommand{\E}{\ensuremath{{\mathbb E}}} % expected value
\newcommand{\Var}{\ensuremath{{\mathbb V}}} % variance

\setlength{\parindent}{0pt}

\usepackage{float}

\newfloat{Program}{thp}{lop}[section]
\floatname{Program}{Program Listing}

\begin{document}

\pagestyle{empty}

\bigskip

\begin{figure}[htp]
    \centering
    \includegraphics[width=4cm]{uzh logo 2.png}
    \label{fig:UZH}
\end{figure}

\begin{Large}
	\begin{center}
		\textbf{Digital Tools for Finance}
	\end{center}
\end{Large}

\vspace{2cm}

\begin{large}	
	\begin{center}
		\textbf{Homework 2, due 22th of November, 2022} \vspace{0.1cm} \\ {Prof.\ Dr.\ Marc Paolella } \vspace{2cm} \\ \textbf{Matthias Olieslagers - 22714034} \vspace{2cm}
	\end{center}
\end{large}

\tableofcontents

\newpage

\bigskip
\section{Question 1}

\subsection*{Question statement}
Simulate "rep" repetitions of a data generation process (DGP) of "T" independent and identically distributed (IID) location-scale student t. For each repetition, calculate a bootstrap confidence interval (confidence level: 90\%), based on "B" bootstrap replications. The bootstrap should be performed using the nonparametric as well as the parametric method.
\newline \newline
For each confidence interval that is generated, the following results should be included:
\newline
1) The length of the Confidence Interval (CI) \newline
2) Whether or not the CI contains the true ES \newline \newline
Report the coverage probability and lengths for both bootstrap methods as a function of the sample size T and draw meaningful conclusions from the findings.


\subsection*{Solution - MatLab code and comments}
The MatLab code for Question 1 is provided in two parts: Program Listing \ref{Question 1 - Part 1} and \ref{Question 1 - Part 2} down below. Part 1 describes the first steps: setting parameter values, computing true ES and calculating the Expected Shortfalls through (non)parametric bootstrapping. Part 2 then follows up on this and contains the code for the confidence intervals, the coverage and lengths and finally also the display of the results. The explanation on the first part \ref{Question 1 - Part 1} is provided here below.
\newline \newline
In order to come to the solution, several steps should be taken. First, the parameter values for location, scale and degrees of freedom (df) are set (loc=1,scale=2,df=4 in this case). Furthermore, the sample size T (the length of each DGP sequence), amount of repetitions rep and bootstrap replications B are also defined upfront. I played around with these values for rep and B a decided to put B at 500 and rep at 250 throughout most of the exercises in the assignment. This choice resulted from a trade-off between accuracy and computing speed. I felt that it was important to have a high enough B to have accurate bootstrapping, but that it would be possible to get good enough results and save time by setting rep a bit lower at 250. \newline Next, the true theoretical ES based on these parameter values is calculated, the code for which was provided by prof. Marc Paolella. It is important to note that, throughout this whole assignment, an alpha value of 0.05 is used for the Expected Shortfall. This theoretical ES is used as a reference point in later parts to verify how well the bootstrap algorithms actually work. In other words, we will check in a later phase whether the bootstrap confidence intervals contain the true ES or not.\newline
Up next, four different vectors (each of size "rep", initially filled with zeros) are initiated. These vectors are used to keep track of the lengths and coverage (0 or 1) for both methods. After each repetition, the corresponding cell for that repetition in each of the vectors will be updated.\newline\newline Subsequently, a nested sequence of two loops is written to perform the Data Generation Process and the bootstrapping. In the outer loop (repeats "rep" times) a dataset of "T" IID location-scale student t is generated. Next to that, for every "rep", two vectors of length B (amount of bootstrap replications) are initiated that will store the list of Expected Shortfalls (ES) that are calculated within each inner loop.\newline 
In the inner loop, that repeats itself "B" times for each "rep", the actual bootstrap replications are performed and the Expected Shortfall is calculated for each bootstrap replication. As mentioned before, this is done in a non parametric as well as in a parametric manner. \newline \newline
For the nonparametric bootstrap, the 'randi' function is used to resample (with replacement) T values from the initial data set. Based on this resampled bootstrap data set, the ES is then calculated in the empirical way (see also previous assignment). The ES is then added to the vector of ES's for the nonparametric bootstrap at the "j"-th spot. As mentioned before, this vector was initiated in the outer loop already, it has size "B" and exists "rep" times in total. \newline\newline Parametric bootstrapping on the other hand works as follows: the bootstrap resamples from a known distribution function (in this case the T-Location-Scale distribution) with parameters that are estimated from the initial data sample. Therefore, as a first step, the MLE (Maximum Likelihood Estimator) parameters are calculated based on the true dataset "data". In this case, the built-in MatLab mle function is used, but the MLE optimization code provided by prof. Marc Paolella could have also been used equivalently, this in done so in Question 4. Both methods provide the same results and a more elaborate comparison between the two will be provided at the end of this question. \newline These MLE parameter values are stored in the parameters loc2, scale2, df2 respectively and the bootstrap data set is then generated based on the MLE parameter values. Then, the ES is calculated for each bootstrap data set by calculating MLE parameter values again on this bootstrap data set and then calculating the true theoretical ES (same method as above) based on these latest MLE values. 
The MatLab code for this first part, is provided in Program Listing \ref{Question 1 - Part 1}. As mentioned before, the rest of the algorithm follows further in Program Listing \ref{Question 1 - Part 2}.

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 1 -------------------------------------------------------------
% Parameters
loc=1; scale=2; df=4; T=2000; rep=250; B=500; 

% Calculation of the True theoretical ES (as a reference point)
alpha=0.05; c05 = tinv(alpha , df); truec = loc+scale*c05; 
ES05 = -tpdf(c05,df)/tcdf(c05,df) * (df+c05^2)/(df-1);
trueES = loc+scale*ES05; 

Coverage_vec_nonparam = zeros(rep,1);Lengths_vec_nonparam = zeros(rep,1);
Coverage_vec_param = zeros(rep,1);Lengths_vec_param = zeros(rep,1);
for i=1:rep
  data=loc+scale*trnd(df,T,1); 
  % bootstrap loop (non parametric and parametric)
  ES_vec_bootstrap_nonparam = zeros(B,1); ES_vec_bootstrap_param =zeros(B,1);
  for j=1:B
      % non parametric bootstrap
      index = randi(T,[T,1]); nonparam_bootstrap_data = data(index);
      VaR=quantile (nonparam_bootstrap_data, alpha); 
      temp=nonparam_bootstrap_data(nonparam_bootstrap_data<=VaR); 
      ES_vec_bootstrap_nonparam(j)=mean(temp);
      % parametric bootstrap
      % compute the MLE parameters on the initial data set "data"
      MLE_parameters = mle(data,'Distribution','tlocationscale');
      loc2 = MLE_parameters(1); scale2 = MLE_parameters(2); 
      df2 = MLE_parameters(3);
      % generate bootstrap data based on the MLE parameters loc2,scale2,df2
      param_bootstrap_data = loc2+scale2*trnd(df2,T,1); 
      % calculate the ES for each bootstrap data set 
      MLE_parameters_bootstrap = mle(param_bootstrap_data,'Distribution','tlocationscale');
      loc3 = MLE_parameters_bootstrap(1); scale3 = MLE_parameters_bootstrap(2); df3 = MLE_parameters_bootstrap(3);
      c05_boot = tinv(alpha , df3); truec_boot = loc3 + scale3*c05_boot; 
      ES05_boot = -tpdf(c05_boot,df3)/tcdf(c05_boot,df3) * (df3+c05_boot^2)/(df3-1);
      ES_vec_bootstrap_param(j) = loc3+scale3*ES05_boot;     
  end

\end{lstlisting}
\caption{Question 1 - Part 1}
\label{Question 1 - Part 1}
\end{Program}

\newpage
As mentioned above, Question 1 is far from over yet. The remainder of the code can be found below in Program Listing \ref{Question 1 - Part 2}.\newline At the end of Part 1, the inner loop (the actual bootstrapping) was closed. Part 2 picks back up right where it was left off and continues the code in the outer loop. First, the 90\% Confidence Intervals for the nonparametric and parametric bootstrap are created by using the quantile() function. Next, a series of conditional statements is run to calculate the Coverage vector. These statements basically just verify whether the trueES of the distribution, calculated at the very beginning, falls within the Confidence Interval and produces a 0 ("no) or 1 ("yes) accordingly. This gives an idea of how good the bootstrap ES estimation actually is (the higher the Coverage, the better the bootstrap method works).Of course, it does this for both the non parametric and parametric method and repeats "rep" times. 
Lastly, the lengths of the CI's are then calculated by substracting the end value of the CI with its begin value.\newline \newline
The outer loop is then finally ended and the program is closed by producing the mean of each of the vectors. The mean of the vector that contains the lengths of the CI's gives us of course the average length of the CI for each method and the mean of the Coverage vector gives an idea of the Coverage ratio. The coverage ratio represents the amount of times that the trueES falls within the (non)parametric CI for each rep (as a decimal value).
This output, computed for several different combinations of parameters, is displayed and discussed in the next section on the next page.  

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
% Continuance of Question 1 ----------------------------------------------

  % make the CI (90%) based on the B-sized vectors of Es's
  CI_nonparam = quantile(ES_vec_bootstrap_nonparam, [0.05 0.95]);
  CI_param = quantile(ES_vec_bootstrap_param, [0.05 0.95]); 
 
  % if-else statements to build the Coverage vector containing 0 and 1
  if trueES>=CI_nonparam(1,1) && trueES<=CI_nonparam(1,2) 
      Coverage_vec_nonparam(i)=1;
  else
      Coverage_vec_nonparam(i)=0;
  end
  if trueES>=CI_param(1,1) && trueES<=CI_param(1,2) 
      Coverage_vec_param(i)=1;
  else
      Coverage_vec_param(i)=0;
  end
  % calculate the lengths of each CI
  Lengths_vec_nonparam(i) = CI_nonparam(1,2)-CI_nonparam(1,1);
  Lengths_vec_param(i) = CI_param(1,2)-CI_param(1,1);
end

% finally, use mean() to get average length of CI and coverage ratio
Average_length_CI_nonparam = mean(Lengths_vec_nonparam)
Coverage_ratio_nonparam = mean(Coverage_vec_nonparam)
Average_length_CI_param = mean(Lengths_vec_param)
Coverage_ratio_param = mean(Coverage_vec_param)

\end{lstlisting}
\caption{Question 1 - Part 2}
\label{Question 1 - Part 2}
\end{Program}

\newpage

\subsection*{Solution - Results and interpretation}
The outcome of the two bootstrap methods, in function of the sample size T, can be found in the tables below. The first table compares interval lengths and the second table looks at coverage ratio. For your reference, the following parameter values are used: loc=1, scale=2, df=4 and also rep=250, B=500.
\newline \newline
As was expected in the hypothesis by prof. Paolella, both bootstraps work well (coverage of more than 0.75, even for small sample T = 250), but the parametric does clearly better. It has shorter CI lengths and better coverage, across all sample sizes. \newline A second interesting insight is the fact that, for both types of bootstrap, the length of the interval decreases and the coverage increases as T gets larger. This makes sense of course because the larger T becomes, the better the data generation process will be able to reflect the true underlying distribution. For large enough T (e.g. T = 2000), the coverage ratio (of both bootstrap methods in fact) clearly converges to the  90\% we were aiming for when we initially set the confidence intervals. This is an indication that the program behaves as was expected.\newline 

\begin{center}
\begin{tabular}{||c | c c||} 
 \hline
 Sample size T & Length CI (nonparametric) & Length CI (parametric) \\ [0.5ex] 
 \hline\hline
 250 & 2.5471 & 2.3270 \\ 
 \hline
 500 & 2.1019 & 1.6376 \\
 \hline
 2000 & 1.0723 & 0.8013 \\ [1ex] 
 \hline
\end{tabular}
\end{center}
\vspace{3pt}
\begin{center}
\begin{tabular}{||c | c c||} 
 \hline
 Sample size T & Coverage (nonparametric) & Coverage (parametric) \\ [0.5ex] 
 \hline\hline
 250 & 0.7680 & 0.8680 \\ 
 \hline
 500 & 0.8280 & 0.8760 \\
 \hline
 2000 & 0.8800 & 0.8920 \\ [1ex] 
 \hline
\end{tabular}
\end{center}
\vspace{15pt}

%add this as a bonus 

As was mentioned already before, it is interesting to compare Matlab's mle method with the "native" MLE function that was provided in the book of prof. Paolella. That function, which I called 'MLE-optimization-Q1' in this assignment, is provided in the appendix in Program Listing \ref{Q1 - Function for MLE optimization}. I adapted the order of the parameters in that function to match the order of the output that the built-in MatLab mle method returns (loc,scale,df), so the two outputs can be easily compared. \newline If the snippet below in Program Listing \ref{Question 1 - MLE comparison} is run, it can easily be seen that the first built-in method always returns exactly the same values as the second "native" method. Therefore, it is appropriate to use the built-in MatLab method in the remainder of the assignment. However, for the last question Q4, an adapted version of the "native" MLE optimization function will again be needed to calculate the MLE for the noncentral student T (see later in Q4). 

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
MLE_parameters = mle(data,'Distribution','tlocationscale')

initvector(1) = loc;  initvector(2) = scale; initvector(3) = df; 
MLE_parameters_native = MLE_optimization_Q1(data,initvector)
\end{lstlisting}
\caption{Question 1 - MLE comparison}
\label{Question 1 - MLE comparison}
\end{Program}

\newpage

\section{Question 2}
\subsection*{Question statement}
Recall that the simulation in Question 1 was based on a location-scale student t distribution. In this question however, simulate data from a (location-scale) \textbf{non-central} student t distribution. Apply the same bootstrap algorithms from the previous exercise to this non-central student t dataset and observe whether the previous algorithms also work on this asymmetric data or not.
\newline Try four different (negative) values for the parameter of non-centrality (e.g.: mu = 0,-1,-2,-3) and two different degrees of freedom (df = 3,6) and compare the results for the different input values and the different bootstrap methods.

\subsection*{Solution - MatLab code and comments}
The MatLab code is provided in Program Listing \ref{Question 2 - Part 1}. \newline 
Please note that this is not the complete Matlab code for this exercise, it only contains the code that differs from Question 1, in order to save some space in this report. Anything that is not explicitly mentioned here, has just been coded in the exact same way as before. An explanation on the code is given below, the actual output is displayed in the next section hereafter.\newline \newline
First, as usual, the parameters are set (loc=1, scale=2, df= (3;6), mu=(-3;-2;-1;0), rep=250, B=500, T=(100;500;2000)), this part is ommitted from the Program Listing as it is trivial. Next, the true ES of the NCT is calculated by using the integral definition. This is based on the codes by prof. Marc Paolella, but adapted for use for the NCT by using 'nctinv' and 'nctpdf' and including 'mu' as a parameter. I tested this NCT numeric integration method first by setting mu = 0 and comparing it with the output that is given by the basic true theoretical ES calculation that was used in Question 1, they gave the same results. \newline
Then, a vector that keeps track of the 'true' ES from simulation is initiated. By taking the mean of this vector after completing all the reps, it can be checked whether the empirical ES based on simulation (of the original data, not the bootstrap) is approximately the same as the ES that was calculated through numeric integration. Some illustrations of this will be included in the results in the next section and will show that the two methods always give approximately the same results, especially as T gets larger of course. \newline\newline
Next, the loop with "rep" repetitions is initiated. This time around, sample data has to be generated from a loc-scale NCT instead of a general loc-scale student T. As was explained in the books by prof. Paolella (Fundamental Statistical Interference, p.351), this can be done as follows. First, simulate random instances from a normal distribution with mean 'mu' and standard deviation '1' and then also from a chi-squared distribution with 'df' degrees of freedom, call those X and Y respectively. The NCT is then described by $T = X / (\sqrt{Y/df} )$.  

Similarly as in question 1, we also have to take location and scale into account, and in doing so, this then gives us the equation for data generation of the NCT. Remark that, alternatively, MatLab also has a function nctrnd() that works in a similar fashion as the trnd() function from Question 1, but we were explicitly asked not to use it in this assignment.
The remainder of the bootstrapping mechanism is in fact the same code as in Question 1 and will thus not be repeated here. The output can be found in the tables in the next section.\newline \newline
Interesting to add is the fact that te following Questions 2,3 and 4 generally all took very long to run. As of this question onwards, I therefore often encapsulated the whole program in one big loop that cycled through the different parameter values and let it run overnight, as was suggested by professor Paolella.

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 2 -------------------------------------------------------------
% Calculate the true ES of the NCT with integral definition
cLS= loc + scale*nctinv(alpha , df, mu); 
ILS = @(y) (y).*nctpdf((y-loc)/scale, df,mu)/scale; 
TrueES_NumericInt = integral(ILS , - Inf , cLS) / alpha; 

Vec_trueES_simulation = zeros(rep,1);

for i=1:rep
  X = normrnd(mu,1,T,1); 
  Y = chi2rnd(df,T,1);
  data = loc + scale*(X ./ sqrt(Y ./ df)); 
  
  % calculation of Empirical ES
  VaR=quantile (data, alpha); 
  temp=data(data<=VaR); Vec_trueES_simulation(i)=mean(temp); 
  
\end{lstlisting}
\caption{Question 2 - Part 1}
\label{Question 2 - Part 1}
\end{Program}


\subsection*{Solution - Results and interpretation}

First, as was mentioned before, let's make a quick comparison between the true ES calculated with numeric integration and the empirical ES through simulation for a couple of various examples. These are just some random examples, purely to illustrate that both ES procedures work and give approximately the same results for any kind of values. Furthermore, it can immediately be seen that the deviation between the exact approach and simulation gets significantly smaller as T increases. Logically, because as T increases, the data generation process gets more accurate and therefore the empirical method of calculating ES through simulation gets better and therefore closer to the exact value. \newline \newline
1) \textbf{Example 1}: $mu = 0, df = 3, T = 100$     \newline ---     ES simulation = -6.4912, ES numeric int = -6.7485,  \textbf{Deviation of ES's = 0.2573}  \newline
2) \textbf{Example 2}: $mu = -1, df =3, T = 500$ \newline --- ES simulation = -12.8827, ES numeric int = -12.8923 , \textbf{Deviation of ES's = 0.0096} \newline
3) \textbf{Example3}: $mu = -3, df = 6, T = 2000$  \newline--- ES simulation = -15.6922, ES numeric int = -15.6920 , \textbf{Deviation of ES's = 0.0002}\newline\newline

Next, let's now take a look at the core question of this exercise and see how well the non parametric and parametric bootstrap from Question 1 can hold up when they are applied on a non-central student t dataset.
For your reference, let's mention again that the parameters are set as follows: loc=1, scale=2, , rep=250, B=500. \newline 
The parameters T, df and mu vary as following: T=(100;500;2000), df= (3;6), mu=(-3;-2;-1;0). The tables on the following pages will compare the coverage ratio and lengths of the CI's for each of those combinations of parameters. \newline 

The first two tables down below on next page illustrate the findings for \textbf{3 degrees of freedom}. The first table displays the lengths and the second is about the coverage ratio. The results for 6 degrees of freedom are displayed at a later point in the text. \newline
As was asked, we let T vary from 100 to 500 and 2000 and mu takes values 0,-1,-2,-3. 

\newpage


\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Length CI (nonparametric) & Length CI (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{4}{4em}{T=100} & $\mu$ = 0 & 5.2666 & 8.9319\\ 
& $\mu$ = -1 & 9.6449 & 9.7008 \\ 
& $\mu$ = -2 & 13.2334 & 19.2469 \\ 
& $\mu$ = -3 & 19.2560 & 43.7532 \\ 
 \hline
 \multirow{4}{4em}{T=500} & $\mu$ = 0 & 3.0341 & 2.5200\\ 
& $\mu$ = -1 & 5.5520 & 3.1778 \\ 
& $\mu$ = -2 & 7.9358 & 4.9203 \\ 
& $\mu$ = -3 & 10.3040 & 7.5688 \\ 
 \hline
 \multirow{4}{4em}{T=2000} & $\mu$ = 0 & 1.6885 & 1.2255\\ 
& $\mu$ = -1 & 2.7962 & 1.5368 \\ 
& $\mu$ = -2 & 4.6726 & 2.4437 \\ 
& $\mu$ = -3 & 5.9613 & 3.6360 \\ 
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Coverage (nonparametric) & Coverage (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{4}{4em}{T = 100} & $\mu$ = 0 & 0.6800 & 0.8520\\ 
& $\mu$ = -1 & 0.6440 & 0.6080 \\ 
& $\mu$ = -2 & 0.6480 & 0.5800 \\ 
& $\mu$ = -3 & 0.6880 & 0.6240 \\ 
 \hline
 \multirow{4}{4em}{T = 500} & $\mu$ = 0 & 0.7920 & 0.8800\\ 
& $\mu$ = -1 & 0.8560 & 0.1960 \\ 
& $\mu$ = -2 & 0.8040 & 0.1400 \\ 
& $\mu$ = -3 & 0.8160 & 0.2040 \\ 
 \hline
 \multirow{4}{4em}{T=2000} & $\mu$ = 0 & 0.8880 & 0.8880\\ 
& $\mu$ = -1 & 0.8640 & 0.0000 \\ 
& $\mu$ = -2 & 0.8960 & 0.0000 \\ 
& $\mu$ = -3 & 0.8480 & 0.0040 \\ 
 \hline
\end{tabular}
\end{center}

First, let's look at the table for the lengths of the CI's. The following conclusions can be drawn:
\begin{itemize}
  \item For both bootstrap methods, the length of the CI increases as the non-centrality increases.
  \item For both methods, when the sample size T increases, the length of the CI becomes smaller.
  \item Overall, the length of the CI is smaller for the parametric bootstrap method. Although in this table, this is not the case for T = 100, probably because T is too small. On the next page however (with df = 6) it does give that result also for T = 100 already.\newline
\end{itemize}


Next, let's look at the table for the Coverage Ratio. The following conclusions can be drawn:
\begin{itemize}
  \item For mu = 0 (when there is no non-centrality), the parametric coverage ratio is higher than the nonparametric. This is perfectly in line with the conclusion from Question 1.
  \item For all different values of mu, the nonparametric method consistenly keeps its performance at almost the same level (+- 0.65 for T = 100, +- 0.80 for T = 500, +- 0.87 for T = 2000). It seems that this method is not really affected by the value of mu.
  \item As soon as there is non-centrality, the parametric method will perform poorly. This effect is slightly 'hidden' for small T, but becomes very clear as T increases. This is simply because a larger T allows for a better DGP that more accurately reflects the actual NCT distribution and thus reveals the true weakness of the parametric method.
  \item For the nonparametric method, coverage increases as T increases. For large T's, the coverage approaches the 90\% that was defined in the confidence intervals upfront.
\end{itemize}

\newpage
Let's now take a look at the results for df = 6. I did not compute every parameter combination for T = 2000, because of limited computing resources (as I did this assignment alone) and because the findings were clear already from T = 500 and similar as in the table above with df = 3. The additional insights gained of running all these cases at T = 2000 did not make up for the computing time needed and I therefore decided to omit a few instances. The cases that I did not compute are represented with 'n/a' in the table. These missing values are not super relevant for the final conclusion of this exercise.

\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Length CI (nonparametric) & Length CI (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{4}{4em}{T=100} & $\mu$ = 0 & 2.6481 & 2.5196\\ 
& $\mu$ = -1 & 3.6423 & 2.9212 \\ 
& $\mu$ = -2 & 4.9067 & 3.9172 \\ 
& $\mu$ = -3 & 5.9148 & 4.8387 \\ 
 \hline
 \multirow{4}{4em}{T=500} & $\mu$ = 0 & 1.4005 & 1.0989\\ 
& $\mu$ = -1 & 1.9349 & 1.2212 \\ 
& $\mu$ = -2 & 2.5379 & 1.5534 \\ 
& $\mu$ = -3 & 3.2275 & 2.0002 \\ 
 \hline
 \multirow{4}{4em}{T=2000} & $\mu$ = 0 & n/a & n/a\\ 
& $\mu$ = -1 & n/a & n/a \\ 
& $\mu$ = -2 & n/a & n/a \\ 
& $\mu$ = -3 & 1.6814 & 0.9832 \\ 
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Coverage (nonparametric) & Coverage (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{4}{4em}{T=100} & $\mu$ = 0 & 0.7200 & 0.8360\\ 
& $\mu$ = -1 & 0.7040 & 0.6000 \\ 
& $\mu$ = -2 & 0.7680 & 0.5560 \\ 
& $\mu$ = -3 & 0.6800 & 0.4640 \\ 
 \hline
 \multirow{4}{4em}{T=500} & $\mu$ = 0 & 0.8440 & 0.8720\\ 
& $\mu$ = -1 & 0.8840 & 0.2440 \\ 
& $\mu$ = -2 & 0.8600 & 0.1080 \\ 
& $\mu$ = -3 & 0.8280 & 0.0880 \\ 
 \hline
 \multirow{4}{4em}{T=2000} & $\mu$ = 0 & n/a & n/a\\ 
& $\mu$ = -1 & n/a & n/a \\ 
& $\mu$ = -2 & n/a & n/a \\ 
& $\mu$ = -3 & 0.8920 & 0.0000 \\ 
 \hline
\end{tabular}
\end{center}

First and foremost, it is very important to note that all of the findings from the previous table with three degrees of freedom, also still hold here for six degrees of freedom! This gives some more credibility to these previous results. \newline \newline 
Besides that, an important and very clear difference between df = 3 and df = 6 is the fact that six degrees of freedom consistently result in much smaller confidence intervals, which is obviously to be desired because it allows for a much more precise estimation of ES.\newline \newline


\newpage
Finally, as an additional remark, it is interesting to take a look back at the use of the nctpdf function. I used the built-in nctpdf() function in MatLab throughout this exercise. It is worth noting however, that there is also a function to calculate the density of the NCT mentioned in the books of prof. Paolella. This function can be found in the appendix in Program Listing \ref{Q2 & Q4 - Function NCTPDF}. In order to be 100\% sure that both functions actually give the same result, I made a plot, Figure \ref{fig:NCTPDF}, down below. In this plot, the density of the NCT is calculated in the two different ways for several mu values (-3,-2,-1,0). As can be seen, for each mu value, the NCT density that is calculated with the built-in MatLab code (blue line) falls exactly together with the density calculated with the function from the book (green dotted line). This illustrates that both functions can be used to get to the same results for the density of an NCT.\newline  \newline
One small interesting note to add is the fact that the exponential had to be taken at the end for the function in the book in order to come up with the same results as the built-in method. This is because the function in the book actually calculates a logdensity instead of the actual density.

\begin{figure}[!htb]
   \includegraphics[scale=0.5]{nctpdf.jpg}
   \centering
   \caption{NCTPDF comparison}
   \label{fig:NCTPDF}
\end{figure}

\newpage
\section{Question 3}
\subsection*{Question statement}
This question is similar to Question 2. Instead of simulating from the non-central student t, you should now simulate from the symmetric stable paretian distribution. Work with a = 1.6 and a = 1.8 and verify again how well the bootstrap algorithms perform.


\subsection*{Solution - MatLab code and comments}

The largest part of the code is again mostly a replica from Question 1. Only the beginning, namely the DGP is different. The code is provided  below in Program Listing \ref{Question 3 }.\newline
First, the parameters for the symmetric stable Paretian are set. In order to be symmetric, 'b' should be zero. Alpha ('a') is first set to 1.6 and then afterwards to 1.8 to compare the results. All the other values ('rep', 'B', etc.) are the same as in previous exercises. To avoid confusion, I used the parameter 'xi' to indicate the significance level of the ES calculation. In previous exercises, I used 'alpha' for this significance level, but this would have become way too confusing since alpha is also used in this context as a parameter for the symmetric stable paretian.\newline\newline
Next, the true ES is calculated using the Stoyanov Material. The asymstableES function (see appendix: Program Listing \ref{Q3 - Function AsymstableES}), which is itself based on a.o. the Stoy and asymstab function, is used for this. This was extensively discussed in the previous assignment, so I will not dig too deep into this. For each rep then, a data sample is generated by using the built-in random() function on a "Stable" distribution with the corresponding parameters listed above and using a sample size T.The remainder of the code is identical as before, the results are displayed and described in the next section. 

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 3 -------------------------------------------------- 
a = 1.6; b = 0; mu = 0; sigma = 1; xi = 0.05;

TrueES = asymstableES(xi,a,b,mu,sigma);

for i=1:rep
  data = random("Stable",a,b,sigma,mu,T,1);
\end{lstlisting}
\caption{Question 3 }
\label{Question 3 }
\end{Program}



\newpage
\subsection*{Solution - Results and interpretation}
The first table shows the lengths of the confidence intervals, the second table shows the coverage.  

\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\alpha$ & Length CI (nonparametric) & Length CI (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{2}{4em}{T=100} & $\alpha$ = 1.6 & 5.8115 & 5.3171\\ 
& $\alpha$ = 1.8 & 3.8878 & 2.3502 \\ 

 \hline
 \multirow{2}{4em}{T=500} & $\alpha$ = 1.6 & 4.6446 & 1.9519\\ 
& $\alpha$ = 1.8 & 2.3026 & 0.9539 \\ 

 \hline
 \multirow{2}{4em}{T=2000} & $\alpha$ = 1.6 & 3.2763 & 0.9598\\ 
& $\alpha$ = 1.8 & 1.2367 & 0.4617 \\ 
 
 \hline
\end{tabular}
\end{center}

%%%%
\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\alpha$ & Coverage (nonparametric) & Coverage (parametric) \\ [0.5ex] 
 \hline\hline
 \multirow{2}{4em}{T=100} & $\alpha$ = 1.6 & 0.4840 & 0.6320\\ 
& $\alpha$ = 1.8 & 0.5720 & 0.6720 \\ 

 \hline
 \multirow{2}{4em}{T=500} & $\alpha$ = 1.6 & 0.6160 & 0.4680\\ 
& $\alpha$ = 1.8 & 0.6960 & 0.6400 \\ 

 \hline
 \multirow{2}{4em}{T=2000} & $\alpha$ = 1.6 & 0.7080 & 0.1320\\ 
& $\alpha$ = 1.8 & 0.7320 & 0.4320 \\ 
 
 \hline
\end{tabular}
\end{center}

First, let's look at the table for the lengths of the CI's. The following conclusions can be drawn:
\begin{itemize}
  \item For both bootstrap methods, the length of the CI decreases as the stability parameter alpha increases.
  \item For both methods, when the sample size T increases, the length of the CI becomes smaller. Remark that we also had this conclusion in Question 2.
  \item Overall, the length of the CI is smaller for the parametric bootstrap method. Remark that we also had this conclusion in Question 2\newline
\end{itemize}

Next, let's look at the table for the Coverage Ratio. The following conclusions can be drawn:
\begin{itemize}
  \item For both values of alpha, the nonparametric bootstrap does better than the parametric bootstrap. However, this weakness of the parametric bootstrap again (similar as in Q2) only becomes clear as T is taken large enough, it is not shown for example for T = 100.
  \item For a larger alpha value (a=1.8), the parametric bootstrap coverage is better than on its counterpart with smaller alpha value (a=1.6).
  \item For the nonparametric method, coverage increases as T increases. However, for the particular values in this example, the coverage ratio does not fully reach up to the 0.90 mark (which it clearly did in Question 2). 
\end{itemize}

The general conclusion for both Question 2 and 3 is that the nonparametric bootstrap works relatively well or even extremely well, whereas the parametric bootstrap generally does not. This conclusion becomes especially clear with large sample sizes T.
The key difference between Question 2 and Question 3 seems to be the following. In the asymmetric case in Question 2, the nonparametric method did extremely well and the parametric method did extremely poor, whereas here in the symmetric case in Question 3, the nonparametric method did good (but not great), but more importantly, the parametric method did not as extremely bad as was the case in Question 2, especially for higher alpha value. Nevertheless, the parametric method still did not produce good results at all, just slightly better as before.


\newpage

\section{Question 4}
\subsection*{Question statement}

Question 4 is another variation on the same principle. Similar as in Question 2, you again have to use the NCT for the true data generation process. However, this time around, you should also adapt your parametric bootstrap method in such a way that it correctly uses the NCT distribution instead of the central student T.\newline
You'll need to compute the MLE of the NCT to obtain this result. Make sure to implement two different ways of calculating the density of the NCT (nctpdf) in this MLE calculation.

\subsection*{Solution - MatLab code and comments}

The code for this exercise is written out in Program Listings \ref{Question 4 - Part 1}, \ref{Question 4 - Part 2} and \ref{Question 4 - NCT MLE optimization}. \newline The first part is for the parameters, the 'true' ES calculation and the Data Generation Process. The second part shows the adapted implementation of the parametric bootstrap. The last part is a more detailed look on the NCT MLE optimization function that is used throughout this exercise.\newline
First of all, remark that location is set at 0 and scale is set at 1 for this exercise. This is different than in the first questions where location was set at 1 and scale at 2. The loc and scale parameters however remain present as parameters in the solution, to be able to account for all potential distributions with different loc/scale. \newline\newline
Something else that is very important to mention upfront, is the fact that the computing time for this algorithm took a lot longer than for Q2 and Q3. This is amongst others due to the specific resource intensive programs for NCT MLE optimization and NCT density calculation, that now had to be run for every bootstrap occurence. \newline
Since I am completing this assignment individually and have limited computing resources at hand, I was forced to lower B to 250 instead of 500 (which still produces relatively accurate bootstrap results) and bring the amount of reps even down from 250 to only 50. I decided to bring reps down by so much because it is less important than T and B for the accuracy of the results. However, still, as a consequence of this, the results will definitely not be as accurate as before. \newline \newline I decided furthermore to only compute the exercise for three degrees of freedom, assuming that the results for six degrees of freedom will be largely similar. Lastly, I only worked with T = 500, the previous exercises have shown that this T of 500 is a relatively large enough sample size to see some meaningful results. T = 2000 would simply have taken too long for my PC in this case (It took a complete night for T = 500 already) and T = 100 was shown in previous exercises to be not accurate enough. Having pointed out these important details regarding the context, let's take a look at the code itself.
\newline
\newline

The first part of the code can be found below in Program Listing \ref{Question 4 - Part 1}.
After setting the parameters first (the values for those parameters are discussed above), remark that the rest of the first part of the code is essentially identical to Question 2 (see Program Listing \ref{Question 2 - Part 1}). The corresponding explanation is identical as was provided there. 

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 4 - Part 1 --------------------------------------------------
% Parameters
loc=0; scale=1; df=3; mu=-3; % mu=-3,-2,-1,0
alpha = 0.05; T=500; rep=50;  B=250;
    
% Calculate the true ES of the NCT with integral definition
cLS= loc + scale*nctinv(alpha , df, mu); 
ILS = @(y) (y).*nctpdf((y-loc)/scale, df,mu)/scale; 
TrueES_NumericInt = integral(ILS , - Inf , cLS) / alpha; 

Vec_trueES_simulation = zeros(rep,1);

for i=1:rep
  X = normrnd(mu,1,T,1); 
  Y = chi2rnd(df,T,1);
  data = loc + scale*(X ./ sqrt(Y ./ df)); 
  
  VaR=quantile (data, alpha); 
  temp=data(data<=VaR); Vec_trueES_simulation(i)=mean(temp); 
\end{lstlisting}
\caption{Question 4 - Part 1}
\label{Question 4 - Part 1}
\end{Program}
\newpage

The second part of Question 4 can be found below in Program Listing \ref{Question 4 - Part 2} on the next page. I only provided the code for the adapted version of the parametric bootstrap (within the inner loop), because this is the only part that differs from the initial implementation. The rest of it, such as the nonparametric method and the calculation of the CI's, remains the same. Let's give a word of explanation on the parametric bootstrap method for NCT.\newline \newline 
The function MLE-optimization-Q4, discussed in the next paragraph and in Program Listing \ref{Question 4 - NCT MLE optimization}, is used to compute the MLE of a location-scale NCT.
The bootstrap data set is then generated based on those MLE parameter values. Remark that, similar as with the initial data generating process for NCT, the bootstrap data set is generated in a similar fashion by using a combination of a normal and a chi-squared distribution. Once the bootstrap data set is made, the ES is then calculated for each bootstrap data set by first applying the MLE optimization function again on the bootstrap data set itself and then calculating the true theoretical ES based on these values. This is the same procedure as in Question 1 essentially.\newline \newline
Because it plays such a key role in this exercise, it is worth including the code for the function that computes the MLE optimization. This code for MLE-optimization-Q4 can be found in Program Listing \ref{Question 4 - NCT MLE optimization}. It is similar to the code for MLE optimization of the general loc-scale student t from Question 1, which is listed in appendix \ref{Q1 - Function for MLE optimization}, but slightly adapted for the NCT. \newline First of all, as mentioned before, this code for NCT MLE optimization is based on the books of prof. Marc Paolella. It essentially works with the negative of the sum of loglikelihoods and then takes the minimum value to come up with the maximum likelihood parameters. In order for the function to work for NCT, I had to adapt two things:\newline 1) The parameter values (4 values instead of 3, including mu)\newline 2) The computation of the loglikelihood in two possible ways (ll)\newline\newline
For the computation of loglikelihood (ll) in Program Listing \ref{Question 4 - NCT MLE optimization}, there are two methods. Both methods are provided in the code below: \newline
1) Method 1: using the built-in MatLab nctpdf() function and taking the log of it. This method is not used in this case to produce the output.\newline
2) Method 2: using the nctpdf function from the book of prof. Paolella (see appendix \ref{Q2 & Q4 - Function NCTPDF}). This is already a logdensity by itself so the log should not be taken anymore. I used this second method to compute the output.
\newpage

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 4 - Part 2 ---------------------------------------------------
      % compute MLE using an adapted version of the MLE optimization function
      initvec(1) = loc;  initvec(2) = scale; initvec(3) = df; initvec(4) = mu;
      MLE_parameters_native = MLE_optimization_Q4(data,initvec);
      loc2 = MLE_parameters_native(1); scale2 = MLE_parameters_native(2); 
      df2 = MLE_parameters_native(3); mu2 = MLE_parameters_native(4);
      
      % generate a bootstrap data set based on the MLE parameters 
      % again use normal and chi2 underlying to generate the data
      Xboot = normrnd(mu2,1,T,1); 
      Yboot = chi2rnd(df2,T,1);
      param_bootstrap_data = loc2 + scale2*(Xboot ./ sqrt(Yboot ./ df2)); 
      
      % Calculate the ES for each bootstrap data set 
      initvec2(1) = loc2;  initvec2(2) = scale2; 
      initvec2(3) = df2; initvec2(4) = mu2;
      MLE_parameters_native_bootstrap = MLE_optimization_Q4(param_bootstrap_data,initvec2);
      
      loc3 = MLE_parameters_native_bootstrap(1); scale3 = MLE_parameters_native_bootstrap(2); 
      df3 = MLE_parameters_native_bootstrap(3); mu3 =MLE_parameters_native_bootstrap(4);
     
      truec_boot = loc3 + scale3*nctinv(alpha , df3, mu3);
      ILSboot = @(y) (y).*nctpdf((y-loc3)/scale3, df3,mu3)/scale3; 
      ES_vec_bootstrap_param(j) = integral(ILS , - Inf , truec_boot) / alpha;    

\end{lstlisting}
\caption{Question 4 - Part 2}
\label{Question 4 - Part 2}
\end{Program}

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%% Question 4 - NCT MLE optimization -----------------------------------
function MLE = MLE_optimization_Q4(x,initvec)
tol=1e-5;
opts=optimset('Disp ', 'none ' , 'LargeScale ' , 'Off ' , ...
'TolFun ' ,tol , 'TolX ' ,tol , 'Maxiter ' ,200) ;
MLE = fminunc(@(param ) tloglik (param,x) , initvec , opts) ;

function ll = tloglik(param,x)
loc=param ( 1 ) ; scale=param ( 2 ) ; df=param ( 3 ) ;mu = param(4);
% remark that the parameters are adapted 
if df<0.01, df=rand ;  end 
if scale<0.01 , scale=rand ; end 

% Method 1 : With built-in matlab nctpdf function:
%ll = -log(scale) + log(nctpdf ( (x-loc)/scale , df , mu)); ll = -sum( ll ) ; 

% Method 2 : With nctpdf function from the books of mr. Paolella:
nctpdf_via_the_book = Q4_nctpdf ( (x-loc)/scale , df , mu);
ll = -log(scale) + nctpdf_via_the_book; ll = -sum( ll ) ;

\end{lstlisting}
\caption{Question 4 - NCT MLE optimization}
\label{Question 4 - NCT MLE optimization}
\end{Program}
\newpage

\subsection*{Solution - Results and interpretation}
The output can be found in the tables below. The first table is again about the CI lengths, the second table about the coverage ratio. As mentioned above, I was only able to simulate a limited set of values because I was working individually and had limited computing resources, but these results below should suffice to produce the main insights. Especially in combination with the earlier exercises where I did compute all of the parameter value combinations. 
\vspace{15pt}
\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Length CI (nonparametric) & Length CI (parametric) \\ [0.5ex] 
 \hline
 \hline
 \multirow{4}{4em}{T = 500} & $\mu$ = 0 & 1.5289 & 1.2735\\ 
& $\mu$ = -1 & 2.5257 & 2.5550 \\ 
& $\mu$ = -2 & 3.7348 & 3.5275 \\ 
& $\mu$ = -3 & 4.8773 & 4.8145 \\ 

 \hline
\end{tabular}
\end{center}

%%%%

\begin{center}
\begin{tabular}{||c | c | c c||} 
 \hline
 Sample size T & Value $\mu$ & Coverage (nonparametric) & Coverage (parametric) \\ [0.5ex] 
 \hline \hline
 \multirow{4}{4em}{T = 500} & $\mu$ = 0 & 0.8200 & 0.9000\\ 
& $\mu$ = -1 & 0.7800 & 0.9400 \\ 
& $\mu$ = -2 & 0.7800 & 0.9000 \\ 
& $\mu$ = -3 & 0.7400 & 0.8800 \\ 
 \hline
\end{tabular}
\end{center}


\vspace{10mm}
First of all, it is worth noting that we see that for the non parametric method, the results are actually very similar to the results in Question 2 (with df=3 and T=500), namely always around 0.80, which was of course to be expected. \newline \newline The big difference with Question 2, is that now, the parametric bootstrap again performs better than the non parametric (for all values of non-centrality ). This was expected because the parametric bootstrap was adapted to work properly with the NCT this time of course. As we would expect, the coverage ratio for the parametric method is close to 90\$. The reason that it goes as high as and even above 90 on one occasion is simply because the "rep" was set too low and therefore the results are slightly misleading and not fully accurate. With a good amount of reps of at least 250, the coverage ratio would be more accurate, more specifically it would be below, but very close to 90\%.
\newline \newline The general conclusion is clear. It is illustrated that the parametric bootstrap again works  better than the nonparametric bootstrap, similar as was the case initially in Q1. \newline

\newpage
\section{Appendix}

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%Q1 - Function for MLE optimization ---------------------------------------
function MLE = MLE_optimization_Q1(x,initvec)
tol=1e-5;
opts=optimset('Disp ', 'none ' , 'LargeScale ' , 'Off ' , ...
'TolFun ' ,tol , 'TolX ' ,tol , 'Maxiter ' ,200) ;
MLE = fminunc(@(param ) tloglik (param,x) , initvec , opts) ;

function ll = tloglik(param,x)
mu=param ( 1 ) ; c=param ( 2 ) ; v=param ( 3 ) ; 
% i changed the order of the parameters to match the 
% order of the built-in matlab mle output
if v<0.01, v=rand ;  end 
if c<0.01 , c=rand ; end 
K=beta ( v /2 ,0.5)*sqrt( v ) ; z=(x-mu) / c ;
ll = -log(c) -log(K) -(( v+1) /2) * log (1 + (z.^2) / v ); ll = -sum( ll ) ;
\end{lstlisting}
\caption{Q1 - Function for MLE optimization}
\label{Q1 - Function for MLE optimization}
\end{Program}


\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize]
%Q2 & Q4 - Function: NCTPDF ------------------------------------------
function pdfln = Q2_nctpdf ( x , nu , gam)
vn2 = (nu + 1) / 2; rho = x .^2;
pdfln = gammaln( vn2 ) - 1/2*log( pi*nu) - gammaln( nu / 2 ) - vn2*log1p ( rho / nu) ;
if ( all (gam == 0) ) , return , end
idx = ( pdfln >= -37) ; 
if (any( idx ))
gcg = gam.^2 ; pdfln = pdfln - 0.5*gcg ; xcg = x .*gam;
term = 0.5*log(2) + log (xcg) - 0.5*log(max( realmin , nu+rho ) ) ;
term ( term == -inf ) = log ( realmin ) ; term ( term == + inf ) = log ( realmax ) ;
maxiter = 1e4 ; k = 0;
logterms = gammaln ((nu+1+k) / 2 ) - gammaln( k+1) - gammaln( vn2 ) + k*term ;
fractions = real (exp( logterms ) ) ; logsumk = log ( fractions ) ;
while ( k < maxiter)
k = k + 1;
logterms = gammaln ( ( nu+1+k ) / 2 ) - gammaln( k+1) - gammaln( vn2 ) + k*term( idx ) ;
fractions = real (exp( logterms-logsumk(idx))) ;
logsumk( idx ) = logsumk( idx ) + log1p ( fractions ) ;
idx(idx) = (abs(fractions) > 1e-4) ; if ( all ( idx == false ) ) , break , end
end
pdfln = real ( pdfln+logsumk) ;
end
\end{lstlisting}
\caption{Q2 - Q4 - Function NCTPDF}
\label{Q2 & Q4 - Function NCTPDF}
\end{Program}

\begin{Program}[!htb]
\begin{lstlisting}[style=Matlab-editor,basicstyle=\mlttfamily\footnotesize] 
%Q3 - Function: AsymstableES --------------------------------------------
function [ES, VaR] = asymstableES (xi, a, b, mu, scale)
if nargin < 3 , b=0; end , if nargin < 4 , mu=0; end
if nargin < 5 , scale = 1; end 

% Get q, the quantile from the S(0 ,1) distribution
opt=optimset('Display', 'off', 'TolX', 1e-6) ;
q=fzero(@(x)stabcdfroot(x,xi,a,b), -6); VaR=mu+scale*q;

if (q == 0)
 t0 = (1 / a) * atan ( b * tan ( pi * a /2) ) ;
 ES = ((2 * gamma((a-1)/a)) / (pi - 2*t0)) * (cos(t0) / cos(a*t0)^(1/a));
 return;
end

ES=(scale*Stoy(q, a, b) / xi)+mu;

end

function diff = stabcdfroot(x, xi, a, b)
[~,F] = asymstab(x, a, b);
diff = F-xi;
end
\end{lstlisting}
\caption{Q3 - Function AsymstableES}
\label{Q3 - Function AsymstableES}
\end{Program}

\end{document}
